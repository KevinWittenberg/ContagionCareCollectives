---
title: "AIO paper 2"
author: "Kevin Wittenberg"
date: "5-10-2023"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(1337)
```

```{r, dependencies, results='hide', warning=FALSE}
#Reading/writing data files
library(readxl)
library(feather)
library(XML)
library(xml2)

#Data manipulation
library(plyr)
library(tidyverse)
library(magrittr)
library(stringr)
library(tidyr)
library(dplyr)
library(igraph)

#Modelling and visualization
library(caret)
library(xgboost)
library(DiagrammeR)
library(ROSE)
library(DALEX)
library(ggplot2)

#Parallel processing
library(foreach)
library(parallel)
library(doParallel)

#Spatial analysis
library(cbsodataR)
library(tidyverse)
library(sf)
library(spdep)
```

# User-defined variables - input required
```{r}
#Set this path equal to the location of the cloned respository (i.e. the highest-level folder of the project)
path <- "C:/Users/5510988/Documents/TEMP P2"

#Set this to TRUE if you have obtained access to the NLZVE survey data, otherwise set it to FALSE. Permission may be obtained from the corresponding author, it is not included in the default replication package.
NLZVE_access <- TRUE
```

Define other paths based on the user data
```{r}
path_elec <- file.path(path, "GR2018")
path_raw <- file.path(path, "rawdata")
path_clean <- file.path(path, "cleandata")
if (NLZVE_access == TRUE) {
  path_nlzve <- file.path(path, "nlzve")
}
```


Open locational data of citizen collectives for care (as well as a dummy indicating municipal co-occurence)
```{r, data import 1}
df_care <- read.csv(file.path(path_raw, "ccnl.csv"))
df_care_multi <- read_feather(file.path(path_raw, "cc_multi"))
df_care$X <- NULL
```

# Municipality election data import

Opening a single EML file and parsing its contents into absolute and relative vote counts per party
```{r}
parties <- c("SP (Socialistische Partij)", "CDA", "Partij van de Arbeid (P.v.d.A.)", "Democraten 66 (D66)", "ChristenUnie", "VVD", "GROENLINKS", "PVV (Partij voor de Vrijheid)", "50PLUS", "Staatkundig Gereformeerde Partij (SGP)", "Partij voor de Dieren", "DENK", "local_other")

parties <- gsub(" ", "_", parties)
parties <- gsub("\\(", "", parties)
parties <- gsub(")", "", parties)

result_total <- data.frame(matrix(nrow=0, ncol=(length(parties)+1)))
colnames(result_total) <- c("municipality", parties)

folders <- list.files(path_elec)
for (folder in folders) {
  files <- list.files(file.path(path_elec, folder))
  files <- files[startsWith(files, "Totaaltelling")]
  
  for (file in files) {
    #Open EML file as r list object
    xml <- read_xml(file.path(path_elec, folder, file)) 
    xml_list <- as_list(xml)
    
    #Record municipality name
    name <- as.character(xml_list$EML$Count$Election$ElectionIdentifier$ElectionDomain[[1]])
    
    #Subset list to information of interest
    counts <- xml_list$EML$Count$Election$Contests$Contest$TotalVotes

    #Initialize empty dataframe to store election results per party
    result_municipality <- data.frame(matrix(ncol=2,nrow=0, dimnames=list(NULL, c("party", "votes"))))
    
    for (i in 1:length(counts)){
      df_t <- data.frame(matrix(ncol=2,nrow=1, dimnames=list(NULL, c("party", "votes"))))
      if (length(counts[i]$Selection$AffiliationIdentifier$RegisteredName) > 0) {
        df_t[1, "party"] <- counts[i]$Selection$AffiliationIdentifier$RegisteredName[[1]]
        df_t[1, "votes"] <- counts[i]$Selection$ValidVotes[[1]]
        result_municipality <- rbind(result_municipality, df_t)
      }
    }
    
    #Prepare string data for reshaping
    result_municipality$party <- gsub(" ", "_", result_municipality$party)
    result_municipality$party <- gsub("\\(", "", result_municipality$party)
    result_municipality$party <- gsub(")", "", result_municipality$party)
    
    #Reshape municipality results
    rownames(result_municipality) <- result_municipality[,1]
    result_municipality <- data.frame(t(result_municipality))
    result_municipality <- result_municipality[-1,]
    result_municipality <- data.frame(lapply(result_municipality, as.numeric))
    
    #Combine local and other parties into single category
    df_other <- result_municipality[!(colnames(result_municipality) %in% parties)]
    result_municipality['local_other'] <- rowSums(df_other)
    result_municipality <- result_municipality[colnames(result_municipality) %in% parties]
    
    #Add municipality name
    result_municipality$municipality <- name
    
    result_total <- rbind.fill(result_total, result_municipality)
  }
}

#Record votes as percentage of total votes cast and include total votes cast as variable
result_total$total_votes <- rowSums(result_total[,-1], na.rm = TRUE)
result_total[is.na(result_total)] <- 0
result_total_perc <- result_total
  
result_total_perc[,-1] <- sapply(result_total[,-1], function(x) x/result_total$total_votes)
result_total_perc$total_votes <- result_total$total_votes
```

Add left-right scale of average political orientation to the election file
```{r}
#Left-right scale
parties <- names(result_total_perc[-c(1, length(names(result_total_perc)))])

scores <- c(2.9, 5.5, 3.4, 4.4, 5.2, 6.3, 2.4, 7.4, 5.1, 5.8, 3.6, 4.8, 4.733)

# Create a named vector of party scores
party_scores <- setNames(scores, parties)

# Calculate the left-right score for each municipality
result_total_perc$left_right_score <- rowSums(result_total_perc[, parties] * party_scores)
```

Link the municipality election outcomes between municipality codes and municipality names to make them match to the other data sources
```{r}
#Match municipality names to municpality codes

#Open linking file from 2020 municipality overview by CBS
df_gemlink <- read_excel(file.path(path_raw, "Gemeenten alfabetisch 2020.xlsx"))

df_gemlink %<>%
  select(Gemeentecode, Gemeentenaam) %>%
  rename(gemcode = Gemeentecode, municipality = Gemeentenaam)

#Combine all data
df_election <- merge(df_gemlink, result_total_perc, by = "municipality", all.x = TRUE, all.y = TRUE)
  
#Check which values occur only in one of 2 dataframes
sum(is.na(df_election$gemcode)) #2 cases that exist in election data but not in overview data
sum(is.na(df_election$total_votes)) # 26 municipalities that exist without election data

df_election[is.na(df_election$total_votes),]
df_election[is.na(df_election$gemcode),]


df_election[df_election$municipality == "Bergen (L)", "gemcode"] <- df_election[df_election$municipality == "Bergen (L.)", "gemcode"]
df_election[df_election$municipality == "Bergen (NH)", "gemcode"] <- df_election[df_election$municipality == "Bergen (NH.)", "gemcode"]

df_election <- df_election %>% 
  filter(!(municipality %in% c("Bergen (L.)", "Bergen (NH.)"))) %>%
  mutate(gemcode = as.numeric(gemcode))


write_feather(df_election, file.path(path_clean, "GR2018_result_total_perc"))
```

Cleanup
```{r}
rm(counts, df_gemlink, df_other, df_t, result_municipality, result_total, result_total_perc, xml, xml_list)
```

# Checkpoint: The file GR2018_result_total_perc may be loaded from here for inspection
```{r}
#result_total_perc <- read_feather(file.path(path_clean, "GR2018_result_total_perc"))
```

# Calculating weighted neighborhood statistics to combine with the zipcode-level data 
```{r}
df_kwb <- read_excel(file.path(path_raw, "kwb-2020.xls"))
df_zip <- read.csv(file.path(path_raw, "2020-cbs-pc6huisnr20200801-buurt", "pc6hnr20200801_gwb.csv"), sep = ";")
```

Municipality unambiguous linking file, change zipcode municipalities if these are ambiguous (e.g. addresses fall in 2 municipalities)
```{r}
df_zip$PC4 <- substr(df_zip$PC6, 1, 4)

#Count how often PC4 house numbers can fall in two municipalities
data_count <- df_zip %>%                            
  group_by(PC4) %>%
  summarise(count = n_distinct(Gemeente2020))
sum(data_count$count > 1)

table(data_count[data_count$count > 1, 'PC4'])

#Majority voting on dominant municipality (large margins everywhere)
table(df_zip[df_zip$PC4 == 1261, 'Gemeente2020'])
df_zip[df_zip$PC4 == 1261, 'Gemeente2020'] <- 376

table(df_zip[df_zip$PC4 == 1724, 'Gemeente2020'])
df_zip[df_zip$PC4 == 1724, 'Gemeente2020'] <- 416

table(df_zip[df_zip$PC4 == 2114, 'Gemeente2020'])
df_zip[df_zip$PC4 == 2114, 'Gemeente2020'] <- 377

table(df_zip[df_zip$PC4 == 4062, 'Gemeente2020'])
df_zip[df_zip$PC4 == 4062, 'Gemeente2020'] <- 281

table(df_zip[df_zip$PC4 == 4197, 'Gemeente2020'])
df_zip[df_zip$PC4 == 4197, 'Gemeente2020'] <- 1960

table(df_zip[df_zip$PC4 == 4715, 'Gemeente2020'])
df_zip[df_zip$PC4 == 4715, 'Gemeente2020'] <- 840

table(df_zip[df_zip$PC4 == 5091, 'Gemeente2020'])
df_zip[df_zip$PC4 == 5091, 'Gemeente2020'] <- 823

table(df_zip[df_zip$PC4 == 5383, 'Gemeente2020'])
df_zip[df_zip$PC4 == 5383, 'Gemeente2020'] <- 796

table(df_zip[df_zip$PC4 == 5504, 'Gemeente2020'])
df_zip[df_zip$PC4 == 2114, 'Gemeente2020'] <- 861

table(df_zip[df_zip$PC4 == 6367, 'Gemeente2020'])
df_zip[df_zip$PC4 == 6367, 'Gemeente2020'] <- 986

table(df_zip[df_zip$PC4 == 6574, 'Gemeente2020'])
df_zip[df_zip$PC4 == 6574, 'Gemeente2020'] <- 1945

table(df_zip[df_zip$PC4 == 6924, 'Gemeente2020'])
df_zip[df_zip$PC4 == 6924, 'Gemeente2020'] <- 226

table(df_zip[df_zip$PC4 == 6961, 'Gemeente2020'])
df_zip[df_zip$PC4 == 6961, 'Gemeente2020'] <- 213

table(df_zip[df_zip$PC4 == 7011, 'Gemeente2020'])
df_zip[df_zip$PC4 == 7011, 'Gemeente2020'] <- 222

table(df_zip[df_zip$PC4 == 7351, 'Gemeente2020'])
df_zip[df_zip$PC4 == 7351, 'Gemeente2020'] <- 200

table(df_zip[df_zip$PC4 == 7693, 'Gemeente2020'])
df_zip[df_zip$PC4 == 7693, 'Gemeente2020'] <- 160

table(df_zip[df_zip$PC4 == 7694, 'Gemeente2020'])
df_zip[df_zip$PC4 == 7694, 'Gemeente2020'] <- 160

table(df_zip[df_zip$PC4 == 7933, 'Gemeente2020'])
df_zip[df_zip$PC4 == 7933, 'Gemeente2020'] <- 118

table(df_zip[df_zip$PC4 == 7963, 'Gemeente2020'])
df_zip[df_zip$PC4 == 7963, 'Gemeente2020'] <- 1690

table(df_zip[df_zip$PC4 == 7964, 'Gemeente2020'])
df_zip[df_zip$PC4 == 7964, 'Gemeente2020'] <- 1690

table(df_zip[df_zip$PC4 == 8096, 'Gemeente2020'])
df_zip[df_zip$PC4 == 8096, 'Gemeente2020'] <- 269

table(df_zip[df_zip$PC4 == 9207, 'Gemeente2020'])
df_zip[df_zip$PC4 == 9207, 'Gemeente2020'] <- 90

table(df_zip[df_zip$PC4 == 9417, 'Gemeente2020'])
df_zip[df_zip$PC4 == 9417, 'Gemeente2020'] <- 1731

table(df_zip[df_zip$PC4 == 9423, 'Gemeente2020'])
df_zip[df_zip$PC4 == 9423, 'Gemeente2020'] <- 1731

rm(data_count)
```

Calculate Wijk statistics weighting matrix
```{r}
#Intersection addresses between PC4 and Wijk
address_counts <- df_zip %>%
  group_by(PC4, Wijk2020) %>%
  summarize(address_count = n()) %>%
  ungroup() 

#Total in PC4
total_PC4 <- df_zip %>%
  group_by(PC4) %>%
  summarize(total_PC4 = n()) %>%
  ungroup()

#Total in Wijk
total_Wijk <- df_zip %>%
  group_by(Wijk2020) %>%
  summarize(total_Wk = n()) %>%
  ungroup()

#Combine these data to create a dyadic file
df_weights <- left_join(address_counts, total_PC4, by="PC4")
df_weights <- left_join(df_weights, total_Wijk, by="Wijk2020")

#Define fraction of PC4 for percentages, and fraction of Wijk for counts
df_weights['weight_perc'] <- df_weights$address_count / df_weights$total_PC4
df_weights['weight_count'] <- df_weights$address_count / df_weights$total_Wk

#clean workspace
rm(address_counts, total_PC4, total_Wijk, df_zip)
```

```{r}
df_kwb <- df_kwb %>%
  filter(recs == 'Wijk') %>%
  mutate(Wijk2020 = str_remove(gwb_code_8, "^0+")) %>%
  select(-c(1:7))

# Replace cells with exclusively a period "." with NA
df_kwb[df_kwb == "."] <- NA
# Replace commas with a period when surrounded by numbers
df_kwb[] <- lapply(df_kwb, function(x) gsub("(\\d),(\\d)", "\\1.\\2", x, perl = TRUE))
#Convert all values to numeric
df_kwb[] <- lapply(df_kwb, as.numeric)

#Combine all data into one frame with the weights
df_weights <- left_join(df_weights, df_kwb, by = "Wijk2020")
count_cols <- names(df_weights)[grepl("^a", names(df_weights), ignore.case = TRUE)]

#Combine the count columns with the weights
df_weights[count_cols] <- df_weights[count_cols] * df_weights$weight_count

#Identify other columns
other_cols <- setdiff(names(df_weights), count_cols)
other_cols <- setdiff(other_cols, names(df_weights[,sapply(df_weights, is.numeric) == FALSE]))

#combine the perc columns with the weights
df_weights[other_cols] <- df_weights[other_cols] * df_weights$weight_perc

df_wijkstats <- df_weights %>%
  group_by(PC4) %>%
  summarise_at(vars(a_inw:ste_oad), list(weighted = sum))

write_feather(df_wijkstats, file.path(path_clean, "df_wijkstats"))

rm(df_weights, df_kwb, df_wijkstats)
```

# Checkpoint: The file df_wijkstats may be loaded from here for inspection
```{r}
#df_wijkstats <- read_feather(file.path(path_clean, "df_wijkstats"))
```

# Machine learning pipeline -> residuals
```{r}
#LOAD IN DATA
data <- readxl::read_xlsx(file.path(path_raw, "pc4_2020_vol.xlsx"), skip=8)
data <- data[-1,]
data <- as.data.frame(lapply(data, as.numeric))
names(data)[1] <- "PC4"

#Add information about location of care collectives
data_cc <- read.csv(file.path(path_raw, "ccnl.csv"))
data <- merge(data, data_cc, by.x = "PC4", by.y = "postcode", all.x = TRUE, all.y = TRUE)

#Add information about municipal election outcomes
data_election <- read_feather(file.path(path_clean, "GR2018_result_total_perc"))
data <- merge(data, data_election, by = "gemcode", all.x = TRUE)

#Add weighted information about Wijk statistics from CBS
data_wijkstats <- read_feather(file.path(path_clean, "df_wijkstats"))
data <- merge(data, data_wijkstats, by="PC4", all.x = TRUE)

rm(data_election, data_wijkstats, data_cc)

data$X <- NULL
data$gem_initiative <- NULL
data$gemcode <- NULL

#Designate target variable
names(data)[names(data) == "initiative"] <- "target"
data$target <- as.factor(data$target == 1)

#Check for noninformative value columns
same_value_cols <- names(data[which(sapply(data, function(x) length(unique(x))) <= 2)])
drops <- same_value_cols[!(same_value_cols %in% c("target"))]
#Remove non-informative oclumn of election data
data <- data[,!names(data) %in% drops]
 
unique_value_cols <- names(data[which(sapply(data, function(x) length(unique(x))) == nrow(data))])


#Create df for PC4 attachment
ID_df <- data.frame(data$PC4)
rownames(ID_df) <- rownames(data)

#Delete PC4 from data
data$PC4 <- NULL

#Clean data before split
clean_kw <- function(data, vars = "all", NumMisCut = 0, verbose = FALSE) {
  # Select numeric variable names
  numvars <- names(data)[sapply(data, is.numeric)]
  
  # Select factor variable names
  factvars <- names(data)[sapply(data, is.factor)]
  
  # Select string variable names
  strvars <- names(data)[sapply(data, is.character)]
  
  if (vars == "all") {
    vars <- colnames(data)
    
    data[, numvars][data[, numvars] < NumMisCut] <- NA
  }
  return(data)
}

data <- clean_kw(data)


#SPLIT DATA INTO TRAINING AND HOLDOUT SET
set.seed(123)
trainIndex <- createDataPartition(data$target, p = 0.8, list = FALSE)
train <- data[trainIndex, ]
valid <- data[-trainIndex, ]


#DATA PREPROCESSING
# define categorical variables
cat_cols <- which(sapply(train, is.factor))

preprocess_train <- function(data, steps, encode_cats=TRUE, cat_tresh=Inf, verbose=FALSE) {
  require(caret)
  supported = c("forest", "BoxCox", "YeoJohnson", "expoTrans", "center", 
                "scale", "range", "knnImpute", "bagImpute", "medianImpute",
                "pca", "ica", "spatialSign", "corr", "zv", "nzv", 
                "conditionalX")
  if (length(setdiff(steps, supported) > 0)){
    warning(paste("Some preprocessing steps are not properly specified or supported yet and have been skipped, please check the following:", 
                  setdiff(steps, supported)))
  }
  
  if (!"target" %in% colnames(data)) {
    stop(print("error: make sure your outcome variable is named \"target\""))
  }
  
  #Fix if any non-numeric print and convert.
  cat_cols <- sapply(data, is.factor) | sapply(data, is.character)
  data[cat_cols] <- lapply(data[cat_cols], as.factor)
  if (verbose & sum(cat_cols > 0)){
    print(paste("Ensuring that", sum(cat_cols), 
                "character or factor variables are set to factors"))
  }
  
  if ("forest" %in% steps) {
    require(missForest)
    steps <- steps[!steps %in% c("forest")]
    preproc <- (preProcess(data, method = steps, verbose = verbose))
    data_preproc <- predict(preproc, data)
    imp_forest <- missForest(data_preproc, variablewise = FALSE, parallelize = "no")
    data_preproc <- imp_forest$ximp
    
    #? Save imputation technique for predict
  }
  
  else{
    preproc <- (preProcess(data, method = steps, verbose = verbose))
    data_preproc <- predict(preproc, data)
  }
  
  #Initialize relevant lists for categorical data
  cat_cols <- colnames(data_preproc)[cat_cols]
  #Initialize list of cols that fall under the threshold for OHE
  OHEcols_train <- list()
  
  #Initialize list of levels for each  column for OHE
  levels_train_ohe <- list()
  
  #Initialize list of levels for each column for loot
  levels_train_loot <- list()
  
  if (encode_cats){
    for (col in cat_cols) {
      if (length(levels(data_preproc[,col])) < cat_tresh) {
        #Add column to list to ensure same treatment for columns in test set
        OHEcols_train <- append(OHEcols_train, col)
        
        #One-Hot-Encoding of the variable
        dummies <- as.data.frame(model.matrix(~data_preproc[,col]-1)[,2:ncol(model.matrix(~data_preproc[,col]-1))])
        levels <- levels(data_preproc[[col]])[-1]
        colnames(dummies) <- paste0(col,".",levels)
        data_preproc <- cbind(data_preproc, dummies)
        
        #Add levels of variable to var-indexed list
        levels_train_ohe[[col]] <- levels
      }
      else {
        # create encoding lookup table from training data
        group_mean <- aggregate(as.logical(data_preproc$target), by = list(data_preproc[[col]]), FUN = mean)
        group_count <- aggregate(data_preproc$target, by = list(data_preproc[[col]]), FUN = length)
        
        names(group_mean) <- c(col, "mean_target")
        names(group_count) <- c(paste(col, "duplicate"), "count")
        
        group_stats <- data.frame(do.call("cbind", list(group_mean, group_count)))
        group_stats <- group_stats[!grepl("duplicate", colnames(group_stats))]
        
        loot_encoding <- sapply(data_preproc[, col], function(x) {
          group_mean <- group_stats$mean_target[group_stats[, col] == x]
          group_count <- group_stats$count[group_stats[, col] == x]
          (sum(group_mean) - data_preproc$target[x] + group_mean * 0.5) / (sum(group_count) - 0.5)
        })
        
        # Replace the original column with the leave-one-out target encoding
        data_preproc[col] <- loot_encoding
        
        #Append levels and means to list for use in test set
        levels_train_loot[[col]] <- data.frame(col = group_stats[[col]], 
                                               mean_target = group_stats$mean_target, 
                                               count = group_stats$count)
      }
    }
  }
  out <- list(data_preproc = data_preproc, 
              preproc_obj = preproc, 
              cat_cols = cat_cols, 
              OHEcols = OHEcols_train, 
              levels_ohe = levels_train_ohe,
              levels_loot = levels_train_loot,
              imp_forest = ifelse(exists("imp_forest", 
                                         inherits = FALSE), 
                                  imp_forest, NA),
              target_mean = mean(as.logical(data_preproc$target)))
  return(out)
}


preprocess_test <- function(data, preproc_train, verbose=FALSE) {
  
  #Expand preproc_train to get relevant attributes for preprocessing
  preproc_obj <- preproc_train$preproc_obj
  cat_cols <- preproc_train$cat_cols
  OHEcols <- preproc_train$OHEcols
  levels_ohe <- preproc_train$levels_ohe
  levels_loot <- preproc_train$levels_loot
  imp_forest <- preproc_train$imp_forest
  target_mean <- preproc_train$target_mean
  
  data[cat_cols] <- lapply(data[cat_cols], as.factor)
  if (verbose & sum(cat_cols > 0)){
    print(paste("Ensuring that", length(cat_cols), "character or factor variables are set to factors"))
  }
  
  if (!is.na(imp_forest)) {
    # impute missing values using the same MissForest model used on the training set
    if (sum(is.na(data)) > 0) {
      require(missForest)
      data <- missForest(data, variablewise = FALSE, parallelize = "no")$ximp
    }
  }
  
  
  
  
  # Apply preprocessing steps
  data <- predict(preproc_obj, data) #!!changed in version 29-06-2023 (moved down)
  
  if (length(OHEcols) > 0) {
    # One-hot-encoding
    for (col in OHEcols) {
      levels <- levels_ohe[[col]]
      levels(col) <- levels
      dummies <- as.data.frame(model.matrix(~data[,col]-1, 
                                            data = data.frame(data[,col]))[,2:ncol(model.matrix(~data[,col]-1))])
      colnames(dummies) <- paste0(col,".",levels)
      data <- cbind(data, dummies)
    }
  }
  
  if (length(levels_loot) > 0) {
    # Leave-one-out target encoding
    for (col in names(levels_loot)) {
      group_stats <- levels_loot[[col]]
      
      print(group_stats)
      
      # replace values with means
      data[[col]] <- sapply(data[[col]], function(x) {
        ifelse(x %in% group_stats$col, 
               group_stats[match(x, group_stats$col), "mean_target"], 
               target_mean)
      })
      
      print(data[[col]])
    }
  }
  
  return(data)
}

train <- train %>%
  select(-municipality)


train_preproc <- preprocess_train(train, 
                                  steps = c("bagImpute", "center", "scale"), 
                                  encode_cats = FALSE, verbose = TRUE)

#Extract function info to create preprocessed training data in dataframe
train_preproc_df <- train_preproc$data_preproc

#Define automatic ML fitting function
AML_fit <- function(data, train_control, params, refit_best = FALSE, 
                    n_top = 5, metric = "Accuracy") {
  require(plyr)
  require(caret)
  
  # initialize list to store results
  results_list <- list()
  
  #Initialize list for problematic parameters
  border_params <- list()
  
  # loop through each model and parameter grid
  for (i in 1:length(params)) {
    specification <- params[[i]]
    method <- specification$method
    
    if (is.null(specification$param_grid)) {
      fit <- train(target ~ ., data = data, method = method,
                   metric = metric, trControl = train_control)
      
      fit$results$method <- method
      
      results_list[[i]] <- fit$results
      
      #Return border params as non-available for this method
      border_params[[paste0(method)]] <- NA
    }
    
    else {
      fit <- train(target ~ ., data = data, method = method,
                   metric = metric, trControl = train_control,
                   tuneGrid = specification$param_grid)
      
      fit$results$method <- method
      
      results_list[[i]] <- fit$results
      
      best_params <- fit$bestTune
      
      for (param_name in names(specification$param_grid)) {
        if (best_params[[param_name]] == 
            min(specification$param_grid[[param_name]]) &
            length(unique(specification$param_grid[[param_name]])) > 1) {
          print(paste0("Warning: The best-fitting ", method,
                       " model uses the lowest value for the parameter '",
                       param_name, "'."))
          border_params[[paste0(method, ":", param_name,":min")]] <- 
            specification$param_grid[
              which(specification$param_grid[[param_name]] == 
                      min(specification$param_grid[[param_name]])),]
        }
        if (best_params[[param_name]] == 
            max(specification$param_grid[[param_name]]) &
            length(unique(specification$param_grid[[param_name]])) > 1) {
          print(paste0("Warning: The best-fitting ", method,
                       " model uses the highest value for the parameter '",
                       param_name, "'."))
          border_params[[paste0(method, ":", param_name,":max")]] <- 
            specification$param_grid[
              which(specification$param_grid[[param_name]] == 
                      max(specification$param_grid[[param_name]])),]
        }
      }
    }
  }
  
  # combine the results into a single dataframe
  results_df <- do.call(rbind.fill, results_list)
  top_models <- results_df %>% arrange(desc(metric))
  top_momdels <- head(top_models, n_top)
  
  if (refit_best == TRUE) {
    # fit the best model on the full training data
    best_method <- top_models$method[1]
    
    if (is.null(specification$param_grid)) {
      final_fit <- train(target ~ ., data = data, method = best_method,
                         metric = metric, trControl = train_control)
    }
    
    else {
      best_params <- top_models[1, -which(names(top_models) %in% 
                                            c("method", metric, "Kappa", 
                                              paste0(metric,'SD'), "KappaSD"))]
      best_params <- best_params[, apply(best_params, 2, 
                                         function(x) !any(is.na(x)))]
      final_fit <- train(target ~ ., data = data, method = best_method,
                         metric = metric, trControl = train_control,
                         tuneGrid = best_params)
    }
  }
  
  # return the top models and the final fit
  if (refit_best == TRUE) {
    return(list(top_models = top_models, final_fit = final_fit, 
                all_models = results_df, border_params = border_params))
  }
  
  else {
    return(list(top_models = top_models, all_models = results_df, 
                border_params = border_params))
  }
}


# define the parameter grids for each model
param_xgb <- list(method = "xgbTree",
                  param_grid = expand.grid(nrounds = c(5, 7, 10, 15, 17, 19),
                                           max_depth = c(3, 5, 7, 9),
                                           eta = c(0.05, 0.1, 0.2, 0.3),
                                           gamma = c(0.25, 0.5, 1, 1.5, 2),
                                           colsample_bytree = c(0.5, 0.7, 0.9, 0.95),
                                           min_child_weight = c(0, 1, 5),
                                           subsample = c(0.8, 0.85, 0.9)))
param_glm <- list(method = "glm")
param_rf <- list(method = "rf",
                 param_grid = expand.grid(mtry = seq(1:round(sqrt(ncol(train))))))

param_svm <- list(method="svmLinear",
                  param_grid = expand.grid(C = c(0.1, 0.5, 1, 2, 5, 10, 50)))

# combine the parameter grids into a single list
params <- list(param_xgb, param_glm, param_rf, param_svm)

# call the fit_models function
train_control <- trainControl(method = "cv", number = 4, search = "grid")
results_1 <- AML_fit(train_preproc_df, train_control, params, n_top = 5, 
                   refit_best = FALSE)

#Updated results manually
param_xgb <- list(method = "xgbTree",
                  param_grid = expand.grid(nrounds = c(15, 16, 17, 18, 19, 20),
                                           max_depth = c(7, 8, 9, 10),
                                           eta = c(0.05, 0.1, 0.2),
                                           gamma = c(0.5, 0.75, 1, 1.25, 1.5, 2),
                                           colsample_bytree = c(0.65, 0.7, 0.75, 0.8, 0.85, 0.9),
                                           min_child_weight = c(0, 1, 2, 5),
                                           subsample = c(0.8, 0.9, 0.95)))

params_2 <- list(param_xgb)

results_2 <- AML_fit(train_preproc_df, train_control, params_2, n_top = 5, 
                                refit_best = FALSE)

#Updated results manually part 3
param_xgb <- list(method = "xgbTree",
                  param_grid = expand.grid(nrounds = c(17, 18, 19, 20, 21, 22),
                                           max_depth = c(7, 8, 9, 10, 11),
                                           eta = c(0.05, 0.1),
                                           gamma = c(1, 1.25, 1.5, 2),
                                           colsample_bytree = c(0.5, 0.65, 0.75, 0.8, 0.85, 0.9),
                                           min_child_weight = c(0, 1, 2, 5, 6),
                                           subsample = c(0.8, 0.9, 0.95, 0.98)))

params_3 <- list(param_xgb)

results_3 <- AML_fit(train_preproc_df, train_control, params_2, n_top = 5, 
                                refit_best = FALSE)


#Final model tuning
params_train <- data.frame(nrounds=20,
                           max_depth = 9,
                           eta = 0.1,
                           gamma = 1.5,
                           colsample_bytree = 0.8,
                           min_child_weight = 5,
                           subsample = 0.95)

#Fit final model on full training data
fit_train <- train(target~., data = train_preproc_df, method = "xgbTree",
                   metric = "Accuracy", tuneGrid = params_train)

#Evaluate performance on training data
train_pred <- predict(fit_train, train_preproc_df)
train_obs <- as.factor(train_preproc_df$target)

confusionMatrix(train_pred, train_obs)

#Process validation data
head(valid)

valid <- valid %>%
  select(-municipality)

valid_preproc <- preprocess_test(data = valid, preproc_train = train_preproc, 
                                 verbose=TRUE)

#Evaluate performance on test data
valid_pred <- predict(fit_train, valid_preproc)
valid_obs <- as.factor(valid$target)

confusionMatrix(valid_pred, valid_obs)
```
```{r}
#Fit model to entire data for leave-one out estimation
df_residuals <- as.data.frame(matrix(nrow=4069, ncol=3))
colnames(df_residuals) <- c("pred.FALSE", "pred.TRUE", "obs")

data <- data %>%
  select(-municipality)

#Parallel processing of leave-one-out estimation
n.cores <- 5

my.cluster <- parallel::makeCluster(
  n.cores,
  type = 'PSOCK',
  outfile = ""
)

doParallel::registerDoParallel(cl = my.cluster)
foreach::getDoParRegistered()

list_resid_p <- foreach( i = 1:nrow(data)) %dopar% {
  #Seperate left-out row from the dataframe
  df_predcase <- data[i,]
  df_LOOT <- data[-i,]
  
  #Preprocess all other cases
  loot_preproc <- preprocess_train(df_LOOT, 
                                   steps = c("bagImpute", "center", "scale"), 
                                   encode_cats = FALSE)
  
  loot_preproc_df <- loot_preproc$data_preproc
  
  #Fit model to all other cases
  fit_loot <- train(target~., data=loot_preproc_df, method = "xgbTree", 
                    metric = "Accuracy", tuneGrid = params_train)
  
  #Preprocess the left-out observation
  predcase_preproc <- preprocess_test(data = df_predcase, 
                                      preproc_train = loot_preproc, 
                                      verbose=TRUE)
  
  #Register predicted probability and observed value
  predcase_pred <- predict(fit_loot, predcase_preproc, type = "prob")
  predcase_obs <- as.factor(df_predcase$target)
  
  rm(df_predcase, df_LOOT, loot_preproc, loot_preproc_df, fit_loot, predcase_preproc)
  
  return(data.frame("pred" = predcase_pred,
                    "obs" = predcase_obs))
}

df_resid <- do.call('rbind', list_resid_p)

write_feather(df_resid, file.path(path_clean, "df_resid"))
```


```{r}
#Extract residuals
df_resid <- read_feather(file.path(path_clean, "df_resid"))
df_resid[['residual']] <- as.numeric(df_resid$obs) - 1 - df_resid$pred.TRUE

df_resid <- merge(df_resid, ID_df, by=0, all.x=TRUE)
df_resid <- df_resid %>% rename(PC4 = data.PC4) %>% select(-Row.names) %>% arrange(PC4)

#Export residuals for main analyses
write_feather(df_resid, file.path(path_clean, "ML_output"))
```

Setting seed outside of ML pipeline, which was performed on a seperate machine
```{r}
set.seed(1337)
```

Baseline model for comparison
```{r}
cv1 <- train[1:814, "target"]
cv2 <- train[815:1628, "target"]
cv3 <- train[1629:2442, "target"]
cv4 <- train[2443:3256, "target"]

cv_base <- c(mean(as.numeric(cv1)) - 1, mean(as.numeric(cv2)) - 1, mean(as.numeric(cv3)) - 1, mean(as.numeric(cv4)) - 1)

#Mean accuracy for base model
1 - mean(cv_base)

sd(cv_base)
```


#Evaluate ML performance
```{r}
#Evaluate model performances
predfunc <- function(model, newdata) {as.numeric(predict(model, newdata))}
residfunc <- function(model, data, y, predfunc)

#Evaluate model performance and inspect solution
explainer_xgb <- explain(
  model = fit_train,
  data = train_preproc_df,
  y = as.numeric(train_preproc_df$target),
  predict_function = predfunc,
  label = "caret xgb"
)

model_performance(explainer_xgb)


pred = predict(fit.xgb, train_preproc)
obs = train_preproc$target

confusionMatrix(table(pred,obs))

v_xgb <- varImp(fit_train, scale = TRUE)[["importance"]]

v_xgb <- head(v_xgb, 25)

p <- ggplot2::ggplot(v_xgb, aes(x=reorder(rownames(v_xgb),Overall), y=Overall)) +
  geom_point(size=4, alpha=1)+
  geom_segment( aes(x=rownames(v_xgb), xend=rownames(v_xgb), y=0, yend=Overall)) +
  xlab('Variable')+
  ylab('Normalized Importance')+
  theme_minimal() +
  coord_flip() 


saveRDS(fit_train, file.path(path, "fit_seedbest"))

rownames(v_xgb) <- c("Attraction <50km", "Motorbikes (w)", "Age 45_64 (w)", "Gas use (w)", "Relative votes CDA", "Relative votes PVDA", "Gas use (w) 2", "Relative votes local", "pau (w)", "Total votes", "Relative votes VVD", "Electicity use (w)", "Relative votes SP", "Cafe <5km", "m_hh_ver (w)", "p_ov (w)", "p_jz (w)", "a_bed (w)", "Hotels <20km", "a_bed_oq (w)", "Attraction <20km", "a_bed_mn (w)", "Theatre <5km", "a_bst (w)", "Relative votes D66")

```

# Evaluate variability in predictability (this code can only be executed with access to the data by Nederland Zorgt voor Elkaar)
Please contact me if you would like to work with these data. This only applies to the following code chunk:
```{r}
if (NLZVE_access == TRUE) {
  df_nlzve <- read_excel(file.path(path_nlzve, "NLZVE.xlsx"), sheet = 1, trim_ws = TRUE)
  
  df_nlzve %<>% rename(PC4 = V1pt20)
  df_nlzve %<>% mutate(PC4 = as.integer(as.character(PC4)))
  
  df_nlzve <- merge(df_nlzve, df_resid, by="PC4", all.x = TRUE)
  df_nlzve$predlabel <- df_nlzve$pred.TRUE > .5
  
  skim(df_nlzve)
  
  nlzve_num <- names(df_nlzve)[which(sapply(df_nlzve, is.numeric))]
  
  df_pattern <- df_nlzve[c(nlzve_num, "predlabel")] %>% group_by(predlabel) %>% summarise_all(mean)
  
  df_pattern2 <- data.frame(t(df_pattern[-1]))
  colnames(df_pattern2) <- unlist(df_pattern[, 1])
  colnames(df_pattern2) <- c("pred_no", "pred_yes", "pred_NA")
  
  df_pattern2 %<>% mutate(diff = pred_no - pred_yes)
}
```

Clean up after predictability inspection (if available)
```{r}
if (NLZVE_access == TRUE) {
  rm(df_nlzve, df_pattern, df_pattern2)
}
```


#Shape file version with Moran's I
```{r}
shape_PC4 <- read_sf(file.path(path_raw, "CBS_pc4_2020_v1.shp"))

df_resid <- read_feather(file.path(path_clean, "ML_output"))

df_resid$PC4 <- as.integer(as.character(df_resid$PC4))
df_resid[is.na(df_resid$PC4), "PC4"] <- 0
names(df_resid) <- make.names(names(df_resid))

#Append care info
shape_PC4 %<>%
  filter(!(PC4 %in% c("8899", "9166"))) #zipcode 8899 & 9166 are islands so we remove them

shape_PC4 <- merge(shape_PC4, df_care_multi, by.x = "PC4", by.y = "postcode", all.x=TRUE)
shape_PC4[is.na(shape_PC4$initiatives_care), 'initiatives_care'] <- 0
shape_PC4 %<>%
  mutate(initiatives_care_any = ifelse(initiatives_care > 0, 1, 0)) #Identify cases with ANY zipcode for Moran's I
shape_PC4 <- merge(shape_PC4, df_resid, by="PC4", all.x=TRUE)

#Append ML residual info
nb <- poly2nb(shape_PC4, queen=TRUE)
```

Assign neighbours manually for those that have no contact points in their polygons
```{r}
sp.sample <- shape_PC4[c(108, 326, 329, 334, 347, 449, 457, 566, 655, 662, 669, 688, 691, 693, 716, 722, 744, 963, 1011, 1116, 1192, 1260, 1382, 1577, 1578, 1633, 1679, 1769, 1770, 1798, 1860, 2044, 2569, 2667, 2758, 2781, 2821, 3035, 3104, 3248, 3679),]
```

Assign neighbours based on Arcgis map obtained from https://www.arcgis.com/apps/mapviewer/index.html?layers=4bfb07de89954f5c8404b3fa2845c010 using 'Queen' criterion
```{r}
nb[[108]] <- c(which(shape_PC4$PC4 == 1183), which(shape_PC4$PC4 == 1186), which(shape_PC4$PC4 == 1191))
nb[[326]] <- c(which(shape_PC4$PC4 == 1625), which(shape_PC4$PC4 == 1689), which(shape_PC4$PC4 == 1696), which(shape_PC4$PC4 == 1697), which(shape_PC4$PC4 == 1627), which(shape_PC4$PC4 == 1628), which(shape_PC4$PC4 == 1624))
nb[[329]] <- c(which(shape_PC4$PC4 == 1702), which(shape_PC4$PC4 == 1704), which(shape_PC4$PC4 == 1713), which(shape_PC4$PC4 == 1711), which(shape_PC4$PC4 == 1706))
nb[[334]] <- c(which(shape_PC4$PC4 == 1701), which(shape_PC4$PC4 == 1713), which(shape_PC4$PC4 == 1702))
nb[[347]] <- c(which(shape_PC4$PC4 == 1735), which(shape_PC4$PC4 == 1732), which(shape_PC4$PC4 == 1731), which(shape_PC4$PC4 == 1718), which(shape_PC4$PC4 == 1734))
nb[[449]] <- c(which(shape_PC4$PC4 == 1964), which(shape_PC4$PC4 == 1969), which(shape_PC4$PC4 == 1962), which(shape_PC4$PC4 == 1967), which(shape_PC4$PC4 == 1966), which(shape_PC4$PC4 == 1946), which(shape_PC4$PC4 == 1945), which(shape_PC4$PC4 == 1944))
nb[[457]] <- c(which(shape_PC4$PC4 == 1976), which(shape_PC4$PC4 == 1973), which(shape_PC4$PC4 == 1971), which(shape_PC4$PC4 == 1972), which(shape_PC4$PC4 == 2071), which(shape_PC4$PC4 == 2051))
nb[[566]] <- c(which(shape_PC4$PC4 == 2516), which(shape_PC4$PC4 == 2274), which(shape_PC4$PC4 == 2271), which(shape_PC4$PC4 == 2491), which(shape_PC4$PC4 == 2495), which(shape_PC4$PC4 == 2267), which(shape_PC4$PC4 == 2281))
nb[[655]] <- c(which(shape_PC4$PC4 == 2531), which(shape_PC4$PC4 == 2532), which(shape_PC4$PC4 == 2521), which(shape_PC4$PC4 == 2523), which(shape_PC4$PC4 == 2283), which(shape_PC4$PC4 == 2284))
nb[[662]] <- c(which(shape_PC4$PC4 == 2544), which(shape_PC4$PC4 == 2543), which(shape_PC4$PC4 == 2541), which(shape_PC4$PC4 == 2545), which(shape_PC4$PC4 == 2548), which(shape_PC4$PC4 == 2291))
nb[[669]] <- c(which(shape_PC4$PC4 == 2553), which(shape_PC4$PC4 == 2555), which(shape_PC4$PC4 == 2552))
nb[[688]] <- c(which(shape_PC4$PC4 == 2517), which(shape_PC4$PC4 == 2584), which(shape_PC4$PC4 == 2597), which(shape_PC4$PC4 == 2514), which(shape_PC4$PC4 == 2596), which(shape_PC4$PC4 == 2518))
nb[[691]] <- c(which(shape_PC4$PC4 == 2592), which(shape_PC4$PC4 == 2594), which(shape_PC4$PC4 == 2245), which(shape_PC4$PC4 == 2261), which(shape_PC4$PC4 == 2272))
nb[[693]] <- c(which(shape_PC4$PC4 == 2595), which(shape_PC4$PC4 == 2594), which(shape_PC4$PC4 == 2592), which(shape_PC4$PC4 == 2273))
nb[[716]] <- c(which(shape_PC4$PC4 == 2643), which(shape_PC4$PC4 == 2641), which(shape_PC4$PC4 == 2651), which(shape_PC4$PC4 == 2652))
nb[[722]] <- c(which(shape_PC4$PC4 == 2651), which(shape_PC4$PC4 == 2661))
nb[[744]] <- c(which(shape_PC4$PC4 == 2718), which(shape_PC4$PC4 == 2641), which(shape_PC4$PC4 == 2713), which(shape_PC4$PC4 == 2715), which(shape_PC4$PC4 == 2716), which(shape_PC4$PC4 == 2712))
nb[[963]] <- c(which(shape_PC4$PC4 == 3235), which(shape_PC4$PC4 == 3223), which(shape_PC4$PC4 == 3234), which(shape_PC4$PC4 == 3237), which(shape_PC4$PC4 == 3221))
nb[[1011]] <- c(which(shape_PC4$PC4 == 3312), which(shape_PC4$PC4 == 3313), which(shape_PC4$PC4 == 3329), which(shape_PC4$PC4 == 3319))
nb[[1116]] <- c(which(shape_PC4$PC4 == 3552), which(shape_PC4$PC4 == 3554), which(shape_PC4$PC4 == 3565), which(shape_PC4$PC4 == 3564), which(shape_PC4$PC4 == 3562))
nb[[1192]] <- c(which(shape_PC4$PC4 == 3769), which(shape_PC4$PC4 == 3712), which(shape_PC4$PC4 == 3734), which(shape_PC4$PC4 == 3766), which(shape_PC4$PC4 == 3765), which(shape_PC4$PC4 == 3764), which(shape_PC4$PC4 == 3828), which(shape_PC4$PC4 == 3812), which(shape_PC4$PC4 == 3819), which(shape_PC4$PC4 == 3818))
nb[[1260]] <- c(which(shape_PC4$PC4 == 3901), which(shape_PC4$PC4 == 3905), which(shape_PC4$PC4 == 6745), which(shape_PC4$PC4 == 3907), which(shape_PC4$PC4 == 3903))
nb[[1382]] <- c(which(shape_PC4$PC4 == 4207), which(shape_PC4$PC4 == 4241), which(shape_PC4$PC4 == 4247), which(shape_PC4$PC4 == 4161), which(shape_PC4$PC4 == 4212))
nb[[1577]] <- c(which(shape_PC4$PC4 == 4652), which(shape_PC4$PC4 == 4655), which(shape_PC4$PC4 == 4671), which(shape_PC4$PC4 == 4756), which(shape_PC4$PC4 == 4727), which(shape_PC4$PC4 == 4614), which(shape_PC4$PC4 == 4664), which(shape_PC4$PC4 == 4681))
nb[[1578]] <- c(which(shape_PC4$PC4 == 4651))
nb[[1633]] <- c(which(shape_PC4$PC4 == 4813), which(shape_PC4$PC4 == 4814), which(shape_PC4$PC4 == 4811), which(shape_PC4$PC4 == 4819), which(shape_PC4$PC4 == 4837))
nb[[1679]] <- c(which(shape_PC4$PC4 == 4905), which(shape_PC4$PC4 == 4907), which(shape_PC4$PC4 == 4909), which(shape_PC4$PC4 == 4902))
nb[[1769]] <- c(which(shape_PC4$PC4 == 5171), which(shape_PC4$PC4 == 5134), which(shape_PC4$PC4 == 5145), which(shape_PC4$PC4 == 5154), which(shape_PC4$PC4 == 5253), which(shape_PC4$PC4 == 5251), which(shape_PC4$PC4 == 5268), which(shape_PC4$PC4 == 5152), which(shape_PC4$PC4 == 5074), which(shape_PC4$PC4 == 5175))
nb[[1770]] <- c(which(shape_PC4$PC4 == 5151))
nb[[1798]] <- c(which(shape_PC4$PC4 == 5246), which(shape_PC4$PC4 == 5241), which(shape_PC4$PC4 == 5244), which(shape_PC4$PC4 == 5248), which(shape_PC4$PC4 == 5232), which(shape_PC4$PC4 == 5243))
nb[[1860]] <- c(which(shape_PC4$PC4 == 5346), which(shape_PC4$PC4 == 5367), which(shape_PC4$PC4 == 5351), which(shape_PC4$PC4 == 5348))
nb[[2044]] <- c(which(shape_PC4$PC4 == 5703), which(shape_PC4$PC4 == 5702), which(shape_PC4$PC4 == 5761), which(shape_PC4$PC4 == 5704), which(shape_PC4$PC4 == 5751), which(shape_PC4$PC4 == 5756))
nb[[2569]] <- c(which(shape_PC4$PC4 == 7006), which(shape_PC4$PC4 == 7008), which(shape_PC4$PC4 == 7009), which(shape_PC4$PC4 == 7002), which(shape_PC4$PC4 == 7003), which(shape_PC4$PC4 == 7004), which(shape_PC4$PC4 == 7005))
nb[[2667]] <- c(which(shape_PC4$PC4 == 7227), which(shape_PC4$PC4 == 7255), which(shape_PC4$PC4 == 7021))
nb[[2758]] <- c(which(shape_PC4$PC4 == 7482))
nb[[2781]] <- c(which(shape_PC4$PC4 == 7546), which(shape_PC4$PC4 == 7545), which(shape_PC4$PC4 == 7513), which(shape_PC4$PC4 == 7541), which(shape_PC4$PC4 == 7542), which(shape_PC4$PC4 == 7481))
nb[[2821]] <- c(which(shape_PC4$PC4 == 7602), which(shape_PC4$PC4 == 7610), which(shape_PC4$PC4 == 7671), which(shape_PC4$PC4 == 7678), which(shape_PC4$PC4 == 7615), which(shape_PC4$PC4 == 7614), which(shape_PC4$PC4 == 7603))
nb[[3035]] <- c(which(shape_PC4$PC4 == 8051), which(shape_PC4$PC4 == 8014), which(shape_PC4$PC4 == 8016), which(shape_PC4$PC4 == 8191), which(shape_PC4$PC4 == 8013), which(shape_PC4$PC4 == 8055), which(shape_PC4$PC4 == 8131))
nb[[3104]] <- c(which(shape_PC4$PC4 == 8091), which(shape_PC4$PC4 == 8051), which(shape_PC4$PC4 == 8015), which(shape_PC4$PC4 == 8131), which(shape_PC4$PC4 == 8198), which(shape_PC4$PC4 == 8193), which(shape_PC4$PC4 == 8181))
nb[[3248]] <- c(which(shape_PC4$PC4 == 8446), which(shape_PC4$PC4 == 8447), which(shape_PC4$PC4 == 8441), which(shape_PC4$PC4 == 8448), which(shape_PC4$PC4 == 8443), which(shape_PC4$PC4 == 8445))
nb[[3679]] <- c(which(shape_PC4$PC4 == 9311), which(shape_PC4$PC4 == 9312), which(shape_PC4$PC4 == 9301))
```


Reciprocate: i.e. Append the unmapped zipcodes also to the list of neighbors of their newly added links
```{r}
for (i in c(108, 326, 329, 334, 347, 449, 457, 566, 655, 662, 669, 688, 691, 693, 716, 722, 744, 963, 1011, 1116, 1192, 1260, 1382, 1577, 1578, 1633, 1679, 1769, 1770, 1798, 1860, 2044, 2569, 2667, 2758, 2781, 2821, 3035, 3104, 3248, 3679)) {
  
  for (neighbor in nb[[i]]) {
    
  if (i %in% nb[[neighbor]]) {
  nb[[neighbor]] <- nb[[neighbor]]
  } else {
  nb[[neighbor]] <- as.integer(append(nb[[neighbor]], i))
  }
    
  }
}
```

Calculate global Moran's I
```{r}
#Weight matrix
lw <- nb2listw(nb, style="W", zero.policy=TRUE)

#Lagged average care values
care.lag <- lag.listw(lw, shape_PC4$initiatives_care, NAOK = TRUE)

#Moran's I for number of initiatives
moran(shape_PC4$initiatives_care, lw, length(nb), Szero(lw), NAOK = TRUE)[1] #0.37

#Moran's I & joincount for any initiative -> This underlies Table 3 in the paper and test H1
joincount.test(as.factor(shape_PC4$initiatives_care_any), lw)
joincount.multi(as.factor(shape_PC4$initiatives_care_any), lw)

joincount.test(as.factor(shape_PC4$initiatives_care_any), nb2listw(nb, style="B", zero.policy=TRUE))
joincount.multi(as.factor(shape_PC4$initiatives_care_any), nb2listw(nb, style="B", zero.policy=TRUE))

moran(shape_PC4$initiatives_care_any, lw, length(nb), Szero(lw), NAOK = TRUE)[1] #0.25

#Hypothesis test (Monte Carlo) for significant positive spatial clustering
MCMI_num <- moran.mc(shape_PC4$initiatives_care, lw, nsim=999, alternative="greater", zero.policy = TRUE, na.action = na.omit)
MCMI_any <- moran.mc(shape_PC4$initiatives_care_any, lw, nsim=999, alternative="greater", zero.policy = TRUE, na.action = na.omit)

#Residual (Mote Carlo) Moran's I, this underlies Table 3 in the paper and test H2
MCMI_resid <- moran.mc(shape_PC4$residual, lw, nsim=999, alternative="greater", zero.policy = TRUE, na.action = na.omit)
```
Create function to turn neighbors matrix object into an edgelist dataframe to calculate cross-municipal ties
```{r}
neighborsDataFrame <- function(nb) {
  stopifnot(inherits(nb, 'nb'))
  
  ks = data.frame(k = unlist( mapply(rep, 1:length(nb), sapply(nb, length), SIMPLIFY = FALSE) ), k_nb = unlist(nb) )
  
  nams = data.frame(id = attributes(nb)$region.id, k = 1:length(nb) )
  
  o = merge(ks, nams, by.x = 'k', by.y = 'k')
  o = merge(o, nams, by.x = 'k_nb', by.y = 'k', suffixes = c("", "_neigh"))
  
  o[, c("id", "id_neigh")]
}
```

Determine zipcodes on municipality borders
```{r}
zip_link <- data.frame(number = 1:4066)
zip_link <- cbind(zip_link, shape_PC4$PC4)
zip_link %<>% mutate(PC4_ego = shape_PC4$PC4) %>%
  select(c("PC4_ego", "number"))

df_nb <- neighborsDataFrame(nb)
df_nb$id <- as.integer(df_nb$id)
df_nb$id_neigh <- as.numeric(df_nb$id_neigh)

df_ziplink <- merge(df_nb, zip_link, all.x=TRUE, by.x="id", by.y="number")
zip_link %<>% mutate(PC4_alter = PC4_ego) %>%
  select(c("PC4_alter", "number"))
df_ziplink <- merge(df_ziplink, zip_link, all.x=TRUE, by.x="id_neigh", by.y="number")

df_ziplink <- df_ziplink[order(df_ziplink$id),]

df_ziplink <- merge(df_ziplink, df_care[c("postcode", "gemcode")], by.x="PC4_ego", by.y="postcode", all.x=TRUE)
df_ziplink %<>% mutate(gemcode_ego = gemcode) %>% select(-gemcode)
df_ziplink <- merge(df_ziplink, df_care[c("postcode", "gemcode")], by.x="PC4_alter", by.y="postcode", all.x=TRUE)
df_ziplink %<>% mutate(gemcode_alter = gemcode) %>% select(-gemcode)

df_ziplink %<>% mutate(border = (gemcode_ego != gemcode_alter)) %>% filter(!is.na(border))

#1 missing noticed, zipcode 5028, which does not exist currently. Current guess is that it should be 5082
df_localink <- df_ziplink %>% group_by(PC4_ego) %>% summarise(frac_cross = mean(border))
```

Explore local Moran's I values
```{r}
#Local Moran's I for 'raw' binary indicator (not appropriate as a metric)
local_I <- localmoran(shape_PC4$initiatives_care_any, lw, length(nb), Szero(lw), zero.policy = TRUE, na.action = na.omit, alternative = "greater")

df_local_I<- cbind(shape_PC4, local_I)
df_local_I<- merge(df_local_I, df_localink, by.x="PC4", by.y="PC4_ego")

summary(lm(Ii ~ frac_cross, data.frame(df_local_I)))


#Local Moran's I for residuals, these results underlie Table 4 of the paper and test H3
names(shape_PC4) <- make.names(names(shape_PC4))
shape_PC4[is.na(shape_PC4$residual), "residual"] <- 0

local_I_resid <- localmoran(shape_PC4$residual, lw, length(nb), Szero(lw), zero.policy = TRUE, na.action = na.omit, alternative = "greater")

df_local_I_resid<- cbind(shape_PC4, local_I)
df_local_I_resid<- merge(df_local_I_resid, df_localink, by.x="PC4", by.y="PC4_ego")

#Regress residuals on the fraction of cross-municipal neighbors
summary(lm(Ii ~ frac_cross, data.frame(df_local_I_resid)))

```

```{r}
#Error checking for strange local values
df_local_I_resid$moran_normalized <- ifelse(df_local_I_resid$Ii > 1, 1, df_local_I_resid$Ii)
summary(lm(moran_normalized ~ frac_cross, data.frame(df_local_I_resid)))

plot(shape_PC4[which(shape_PC4$PC4 %in% df_high$PC4), "geometry"])
```

Save dataset used for hypothesis testing
```{r}
write_rds(shape_PC4, file.path(path_clean, "df_H1_H2")) #H1
write_rds(df_local_I_resid, file.path(path_clean, "df_H3"))
```

```{r}
export_tableau <- readRDS(file.path(path_clean, "df_H1_H2"))
export_tableau %<>% select(c("PC4", "initiatives_care_any", "residual"))
export_tableau <- data.frame(export_tableau)
export_tableau %<>% select(-geometry)

write.csv(export_tableau, file.path(path_clean, "df_tableau"))
```


AML update function that takes the AML_fit output and allows users to further tune parameters based on boundary fit
```{r}

```


